{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hello, pytorch"
      ],
      "metadata": {
        "id": "lbIJ2AEnj33_",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png)\n",
        "\n\n> **!!!** If you're running this notebook on Colab, make sure you enable the GPU support by going to `'Edit'->'Notebook settings'` and setting `'Hardware accelerator'` to `'GPU'`."
      ],
      "metadata": {
        "id": "tm9CD832ky4o",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[PyTrorch](http://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) are two of the most commonly used deep learning frameworks. Both of these tools are notable for their ability to compute gradients automatically and do operations on GPU, which can be by orders of magnitude faster than running on CPU. Even though both libraries serve the same purpose, they are quite different in the ways they approach calculations.\n",
        "\n",
        "TensorFlow makes your code live in two \"worlds\" simultaneously:  symbolic graphs and actual tensors. First you declare a symbolic \"recipe\" of how to get from inputs to outputs, then feed it with actual data.\n",
        "\n",
        "In pytorch, there's only one world: all tensors have a numeric value. You can compute outputs on the fly without pre-declaring anything, and the code looks (almost) exactly as in pure numpy. Due to this simplicity, PyTorch is going to be our weapon of choice for this course.\n",
        "\n\nWe'll start from using the low-level core of PyTorch, and then try out some high-level features."
      ],
      "metadata": {
        "id": "fea1jESIjHPK",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch\n",
        "print(torch.__version__)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "tNqZu7dvjHPO",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy world\n",
        "\n",
        "x = np.arange(16).reshape(4, 4)\n",
        "\n",
        "print(\"X :\\n{}\\n\".format(x))\n",
        "print(\"X.shape : {}\\n\".format(x.shape))\n",
        "print(\"add 5 :\\n{}\\n\".format(x + 5))\n",
        "print(\"X*X^T  :\\n{}\\n\".format(np.dot(x, x.T)))\n",
        "print(\"mean over cols :\\n{}\\n\".format(x.mean(axis=-1)))\n",
        "print(\"cumsum of cols :\\n{}\\n\".format(np.cumsum(x, axis=0)))"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "scrolled": true,
        "id": "FxIJVjXajHPS",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch world\n",
        "x = np.arange(16).reshape(4, 4) # (starting form a numpy array)\n",
        "\n",
        "x = torch.from_numpy(x).type(torch.float) # or torch.arange(0,16).reshape(4,4)\n",
        "x = x.cuda()\n",
        "#      ^^^ this will move the tensor onto GPU memory\n",
        "\n",
        "print(\"X :\\n{}\".format(x))\n",
        "print(\"X.shape : {}\\n\".format(x.shape))\n",
        "print(\"add 5 :\\n{}\".format(x + 5))\n",
        "print(\"X*X^T  :\\n{}\".format(torch.matmul(x, torch.t(x))))\n",
        "print(\"mean over cols :\\n{}\".format(x.mean(dim=-1)))\n",
        "print(\"cumsum of cols :\\n{}\".format(x.cumsum(dim=0)))"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "U4CU3mBTjHPW",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NumPy and Pytorch"
      ],
      "metadata": {
        "id": "QAOmRYq7ui11",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can notice, pytorch allows you to hack stuff much in the same way you did with numpy. No graph declaration, no placeholders, no sessions. This means that you can *see the numeric value of any tensor at any moment of time*. Debugging such code can be done by printing tensors or using any debug tool you want (e.g. [gdb](https://wiki.python.org/moin/DebuggingWithGdb)).\n",
        "\n",
        "You may also see a few new method names and a different API. So no, there's no compatibility with numpy [yet](https://github.com/pytorch/pytorch/issues/2228) and yes, you'll have to memorize all the names again. Get excited!\n",
        "\n",
        "![img](http://i0.kym-cdn.com/entries/icons/original/000/017/886/download.jpg)\n",
        "\n",
        "For example, \n",
        "* If something takes a list/tuple of axes in numpy, you can expect it to take *args in pytorch\n",
        " * `x.reshape([1,2,8]) -> x.reshape(1,2,8)`\n",
        "* You should swap _axis_ for _dim_ in operations like mean or cumsum\n",
        " * `x.sum(axis=-1) -> x.sum(dim=-1)`\n",
        "* most mathematical operations are the same, but types and shaping is different\n",
        " * `x.astype('int64') -> x.type(torch.LongTensor)`\n",
        "\n",
        "To help you acclimatize, there's a [table](https://github.com/torch/torch7/wiki/Torch-for-Numpy-users) covering most new things. There's also a neat [documentation page](http://pytorch.org/docs/master/).\n",
        "\n",
        "Finally, if you're stuck with a technical problem, we recommend searching [pytorch forumns](https://discuss.pytorch.org/). Or just googling, which usually works just as efficiently. \n",
        "\nIf you feel like you almost give up, remember two things: **GPU** and **gradients for free**. Besides you can always jump back to numpy with x.numpy()."
      ],
      "metadata": {
        "id": "0V4RORsfjHPY",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Warmup: trigonometric knotwork\n",
        "_inspired by [this post](https://www.quora.com/What-are-the-most-interesting-equation-plots)_\n",
        "\n",
        "There are some simple mathematical functions with cool plots. For one, consider this:\n",
        "\n",
        "$$ x(t) = t - 1.5 * cos( 15 t) $$\n",
        "$$ y(t) = t - 1.5 * sin( 16 t) $$\n"
      ],
      "metadata": {
        "id": "4RcZjFrcjHPa",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.linspace(-10, 10, steps = 10000)\n",
        "\n",
        "# compute x(t) and y(t) as defined above\n",
        "x = ###YOUR CODE\n",
        "y = ###YOUR CODE\n",
        "\nplt.plot(x.numpy(), y.numpy());"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "qrekxr8rjHPa",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "if you're done early, try adjusting the formula and see how it affects the plot"
      ],
      "metadata": {
        "id": "fV5oSb_EjHPh",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "_HsquDcmjHPi",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic gradients\n",
        "\n",
        "Any self-respecting DL framework must do your backprop for you. Torch handles this through `requires_grad` parameter when creating tensors.\n",
        "\n",
        "The general pipeline looks like this:\n",
        "* You create ```a = torch.tensor(data, requires_grad=True)```\n",
        "* You define some differentiable `loss = whatever(a)`\n",
        "* Call `loss.backward()`\n",
        "* Gradients are now available as ```a.grad```\n",
        "\n**Here's an example:** let's fit a linear regression on Boston house prices"
      ],
      "metadata": {
        "id": "H2uMMOF7jHPj",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "boston = load_boston()\n",
        "# Let's have a look at the last feature:\n",
        "plt.scatter(boston.data[:, -1], boston.target);"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "scrolled": true,
        "id": "FCp_lBj9jHPk",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll try a linear regression model using this single feature, and we'll use **gradient descent** to optimize the parameters. This is going to be our starting point:"
      ],
      "metadata": {
        "id": "vsCxML8OmX2i",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the parameters of our model:\n",
        "w = torch.zeros(1, requires_grad=True, dtype=torch.float64)\n",
        "b = torch.zeros(1, requires_grad=True, dtype=torch.float64)\n",
        "#         You want to make sure all tensors have    ^^^\n",
        "#         the same dtype\n",
        "\n",
        "# Converting our data to torch tensors:\n",
        "x = torch.from_numpy(boston.data[:,-1]) / 10\n",
        "y = torch.from_numpy(boston.target)\n",
        "\n",
        "# prediction:\n",
        "y_pred = w * x + b\n",
        "# loss:\n",
        "loss = ((y_pred - y)**2).mean()\n",
        "print(\"Loss is:\", loss)\n",
        "\n",
        "# Compute gradients:\n",
        "loss.backward()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "XOFb6pCQl-P2",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradients are now stored in `.grad` of a variable."
      ],
      "metadata": {
        "id": "q1Fy8EXAjHPu",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"dL/dw =\", w.grad)\n",
        "print(\"dL/db =\", b.grad)\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "NuA8Q3Y3jHPv",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you compute gradients from multiple losses, the gradients will add up at variables, therefore you usually want to **zero the gradients** between iteratons."
      ],
      "metadata": {
        "id": "PMXEd1etjHPz",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# let's keep the history of loss values at \n",
        "# each step\n",
        "losses = []\n",
        "\n",
        "# Gradient descent:\n",
        "for i in range(100):\n",
        "  # Calculate current prediction and loss:\n",
        "  y_pred = w * x  + b\n",
        "  loss = ((y_pred - y)**2).mean()\n",
        "  losses.append(loss.item())\n",
        "  #                   ^^^\n",
        "  # The '.item' call returns the value of this \n",
        "  # tensor as a standard Python number (this only works\n",
        "  # for tensors with one element).\n",
        "  \n",
        "  # Compute the gradients\n",
        "  loss.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # ^^^ this `with` statement ensures that\n",
        "    #     automatic gradients will not be computed\n",
        "    #     for whatever is written in the current block:\n",
        "    w -= 0.05 * w.grad\n",
        "    b -= 0.05 * b.grad\n",
        "\n\n",
        "  # manually zero the gradients\n",
        "  w.grad.zero_()\n",
        "  b.grad.zero_()\n",
        "\n",
        "  # the rest of code is just bells and whistles\n",
        "  if (i + 1) % 5 == 0:\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    \n",
        "    # We'll draw the data points and fit result on the left subplot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"data\")\n",
        "    plt.scatter(x.numpy(), y.numpy())\n",
        "    plt.scatter(x.numpy(), y_pred.detach().numpy(),\n",
        "                color='orange', linewidth=5)\n",
        "    # The '.detach()' call is needed here to create a\n",
        "    # copy of 'y_pred' that does not require gradients\n",
        "    # (otherwize it's impossible to get a numpy\n",
        "    # representation).\n",
        "    \n",
        "    # We'll draw the loss (as the function of gradient descent\n",
        "    # step) on the right subplot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"loss\")\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('step')\n",
        "    \n",
        "    plt.show()\n",
        "\n    print(\"loss = \", loss.detach().numpy())"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "TV14F3VejHP0",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Bonus quest__: try implementing and writing some nonlinear regression. You can try quadratic features or some trigonometry, or a simple neural network. The only difference is that now you have more variables and a more complicated `y_pred`. "
      ],
      "metadata": {
        "id": "g8k2EmXwjHP6",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## High-level PyTorch"
      ],
      "metadata": {
        "id": "QjrsEoVl6gm5",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now going to teach the computer to identify some clothes. Exciting, isn't it? The dataset we'll use for that is called [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist). Here's an excerpt from their website:\n",
        "\n",
        "> *Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.*\n",
        "\nSo, if you're familiar with the original [MNIST](http://yann.lecun.com/exdb/mnist/) dataset (handwritten digits), this is kinda the same, but about clothes."
      ],
      "metadata": {
        "id": "xOqcKJuLjD5I",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch has a module dedicated to retrieving datasets, so let's use it:"
      ],
      "metadata": {
        "id": "iegU7EdXkzD4",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import FashionMNIST"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "6usBsSGi6k6u",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the train and test parts of the dataset\n",
        "data_train = FashionMNIST(\"FashionMNIST/\",\n",
        "                          download=True,\n",
        "                          train=True)\n",
        "\n",
        "data_test = FashionMNIST(\"FashionMNIST/\",\n",
        "                          download=True,\n",
        "                          train=False)\n",
        "\n",
        "# In fact, it's already stored as torch tensor, but we'll need\n",
        "# to work with the numpy representation, so let's do the convertion:\n",
        "X_train = data_train.train_data.numpy()\n",
        "y_train = data_train.train_labels.numpy()\n",
        "\n",
        "X_test = data_test.test_data.numpy()\n",
        "y_test = data_test.test_labels.numpy()\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "r49OsLdH6yRo",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The datasets consists of images belonging to one out of 10 classes:\n",
        "\n",
        "| Label | Description | | Label | Description |\n",
        "| ---       \n",
        "| 0        | T-shirt/top   || 5        | Sandal         |\n",
        "| 1        | Trouser        || 6        | Shirt             |\n",
        "| 2        | Pullover       || 7        | Sneaker       |\n",
        "| 3        | Dress           || 8        | Bag              |\n",
        "| 4        | Coat             || 9        | Ankle boot  |\n",
        "\n\n\n\n"
      ],
      "metadata": {
        "id": "p-VvVjxj8IEk",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check out the shapes of our arrays:"
      ],
      "metadata": {
        "id": "yNk7BuyQnHyF",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test .shape, y_test .shape)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "fWq9TWKVCPeE",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we are going to plot images from specific categories"
      ],
      "metadata": {
        "id": "zb9rebjbnNLd",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\n",
        "    X_train[y_train == i]\n",
        "    for i in range(10)\n",
        "]\n",
        "\n",
        "ten_of_each = np.array([c[:10] for c in categories])\n",
        "ten_of_each = np.transpose(ten_of_each, (0, 2, 1, 3)).reshape(280, 280)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(ten_of_each)\n",
        "plt.axis('off');"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "AHvXHn7D7PmY",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, so let's start writing our first deep learning model! For that we'll need 3 things:\n",
        "  - a **loss** function that takes predicted class scores and true labels, and outputs a measure of how wrong we are in our predictions\n",
        "  - a **function** that converts the input features to class scores (this will be represented by a neural network)\n",
        "  - an **optimization algorithm** that'll vary the parameters of our model to minimize the loss"
      ],
      "metadata": {
        "id": "w0vyO5SSpCF7",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss function"
      ],
      "metadata": {
        "id": "AOWxl3nAALEY",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are facing a classification problem, **cross-entropy** (aka negative log likelihood) is a good choice for a loss function:\n",
        "$$\\mathscr{L} = -\\frac{1}{|D|}\\sum_{c\\in C,\\,i\\in D}p^{\\text{true}}_{c,i}\\cdot log\\,\\hat{p}^{\\text{predicted}}_{c,i},$$\n",
        "where $C$ is the set of all classes and $D$ is the set of all objects."
      ],
      "metadata": {
        "id": "NWvwI3snrz2z",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'>Question:</font> *What is $p^{\\text{true}}_{c,i}$ equal to for a given object $i$ of class $c'_i$?*\n",
        "<font color='red'>Question (2):</font> *What will $\\mathscr{L}$ look like for a binary classification case?*"
      ],
      "metadata": {
        "id": "uqVsCmNDCyD6",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our loss function takes in *probabilities*, we have to make sure they add up to 1 for any given object. In general case, the output of a neural net can be anything, but there's a commonly used activation function that automatically normalizes the outputs to add up to 1:\n",
        "\n",
        "$$f(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
        "\nThis is the *softmax function*, and we'll use it as the activation for the very last layer."
      ],
      "metadata": {
        "id": "5cJm2yA4YDU1",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by trying to implement this function (in torch):"
      ],
      "metadata": {
        "id": "N2beWS7gbIND",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return <YOUR CODE HERE>"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "Y6uYTdlybGim",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you're done, run the cell below to check whether your function passes automatic checks."
      ],
      "metadata": {
        "id": "hO9p5Mr-Si_X",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating some simple input\n",
        "dummy_input_1 = torch.tensor([0.5, -10., 4., 1.333])\n",
        "dummy_input_2 = 100 * dummy_input_1\n",
        "\n",
        "# Computing the output with your implementation\n",
        "dummy_result_1 = softmax(dummy_input_1)\n",
        "dummy_result_2 = softmax(dummy_input_2)\n",
        "\n",
        "# Checking the output type\n",
        "assert isinstance(dummy_result_1, torch.Tensor), \\\n",
        "       \"Your function should return a torch tensor\"\n",
        "\n",
        "# Converting everything to numpy\n",
        "dummy_result_1 = dummy_result_1.numpy()\n",
        "dummy_result_2 = dummy_result_2.numpy()\n",
        "\n",
        "# Correct outputs (for the automatic checks):\n",
        "dummy_correct_result_1 = np.array([2.7460692e-02,\n",
        "                                   7.5616998e-07,\n",
        "                                   9.0937322e-01,\n",
        "                                   6.3165329e-02])\n",
        "dummy_correct_result_2 = np.array([0., 0., 1., 0.])\n",
        "\n",
        "# Doing the checks:\n",
        "assert np.allclose(\n",
        "            dummy_result_1,\n",
        "            dummy_correct_result_1\n",
        "       ), \"Test failed!\\n\" \\\n",
        "       \"Looks like something is wrong \" \\\n",
        "       \"with your softmax implementation.\\nCorrect result is:\\n{}\\n\" \\\n",
        "       \"You got:\\n{}\".format(dummy_correct_result_1, dummy_result_1)\n",
        "\n",
        "assert np.isfinite(dummy_result_2).all(), \\\n",
        "       \"Test failed!\\n\" \\\n",
        "       \"Looks like your implementation is unstable to large inputs.\\n\" \\\n",
        "       \"You got:\\n{}\\n\" \\\n",
        "       \"Can you think of a way of regularizing it?\".format(dummy_result_2)\n",
        "\n",
        "assert np.allclose(\n",
        "            dummy_result_2,\n",
        "            dummy_correct_result_2\n",
        "       ), \"Test failed!\\n\" \\\n",
        "       \"Looks like something is wrong \" \\\n",
        "       \"with your softmax implementation.\\nCorrect result is:\\n{}\\n\" \\\n",
        "       \"You got:\\n{}\".format(dummy_correct_result_2, dummy_result_2)\n",
        "\n",
        "# Printout for the case when all is good.\n",
        "print(\"Tests passed!\")"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "OQGL9r8Kbwny",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "```\n",
        "```\n",
        "```\n",
        "```\n",
        "```\n",
        "```\n",
        "```\n"
      ],
      "metadata": {
        "id": "TmRLk-L0UKdq",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Were you able to regularize your softmax implementation?\n",
        "\n",
        "In fact, since we are combining crossentropy and softmax, we'll always take a `log` of the softmax output (see formulas above). This means one more level of numerical regularisation of the final expression can be done. This has already been implemented in the high-level PyTorch `nn` module within the `torch.nn.CrossEntropyLoss` class.\n",
        "\nLet's use it to create our loss function as follows:"
      ],
      "metadata": {
        "id": "F187gI18UOq3",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the loss function:\n",
        "loss_function = torch.nn.CrossEntropyLoss()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "AlX5RdtoXDps",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `loss_function` now takes two arguments as its input: score predictions (output of the last layer, without activation) and the correct labels:\n",
        "\n",
        "$$\\mathscr{L}(x, c) = -\\frac{1}{|D|}\\sum_{i\\in D}log\\,\\frac{e^{x_i^{c_i}}}{\\sum_{j\\in C}e^{x_i^j}},$$\n",
        "\n",
        "where $x_i^j$ is the score for the $i$-th object to belong the $j$-th class, $c_i$ is the true class for $i$-th object; $C$ is the set of all classes and $D$ is the set of all objects\n",
        "\n",
        "Below is an example of how `loss_function` can now be used.\n",
        "\n**Task:** try changing the values of `dummy_x` to make the resulting loss even smaller."
      ],
      "metadata": {
        "id": "9fem8Z1kZLbA",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage (play around with this code)\n",
        "\n",
        "# Think of dummy_x as of output of the last layer of our neural net.\n",
        "# For example, dummy_x below corresponds to a 3-class classification of\n",
        "# 5 objects:\n",
        "dummy_x = torch.tensor(\n",
        "    [[99., -5., -10.], # here we're almost sure it's class 0\n",
        "     [-3.,  8.,   9.], # here we think it's either class 1 or 2, etc\n",
        "     [-3., 15., -10.],\n",
        "     [ 4.,  5.,  -3.],\n",
        "     [-3., 15., -10.]]\n",
        ")\n",
        "# And these are the answers (vector of class ids):\n",
        "dummy_y = torch.tensor([0, 2, 1, 1, 1])\n",
        "\nprint(loss_function(dummy_x, dummy_y))"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "H9Cps7YwVnTc",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural net"
      ],
      "metadata": {
        "id": "XNf0CX-fAZfH",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a loss function, it's time to build our network. Again, there's a bunch of ready to use classes in the `torch.nn` module.\n",
        "\n",
        "Let's start from something really simple - a model without any hidden layers. For such a case we should only have 1 input layer, 1 output layer and a set of linear connections between them:\n",
        "\n",
        "$$\\text{output}_i = \\sum_j w_{ij}\\cdot\\text{input}_j + b_i$$\n",
        "\n",
        "**Question:** how many parameters does this model have?\n",
        "\nWe'll use the `torch.nn.Linear` class to define these connections:"
      ],
      "metadata": {
        "id": "-WFbVrgutsuf",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 28 * 28 # number of pixels per image\n",
        "output_size = 10 # number of classes\n",
        "model = torch.nn.Linear(in_features=input_size,\n",
        "                        out_features=output_size)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "wgvwEXm4vWSX",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now `model` is a function that connects the two layers. Here's an example of how it works:"
      ],
      "metadata": {
        "id": "kwGSePvGyxkj",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can think of dummy_input as of 5 images with all pixels black:\n",
        "dummy_input = torch.tensor(np.zeros(shape=(5, 28 * 28), dtype=np.float32))\n",
        "# Then our output is going to be the 10 class scores for each of the images:\n",
        "dummy_output = model(dummy_input)\n",
        "\n",
        "# Let's have a look at their shape:\n",
        "print(dummy_input.shape)\n",
        "print(dummy_output.shape)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "bb8tip2XyUOz",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One can iterate through the `model`'s parameters with it's `.parameters()` method. Try using it to calculate the number of (scalar) parameters:"
      ],
      "metadata": {
        "id": "pJucbQGHzoQH",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the number of (scalar) parameters:\n",
        "n_parameters = 0\n",
        "for parameter in model.parameters():\n",
        "  <YOUR CODE HERE>\n",
        "\nprint(n_parameters)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "tI266aNXzk-O",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input preprocessing"
      ],
      "metadata": {
        "id": "gSPYLKzJAgzO",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far our data is held as numpy arrays of unsigned byte type, i.e. it lies within a range from 0 to 255. Also, the shape of our input is 3-dimensional (num_images, height, width), while our `model` takes 2-dimensional \"arrays of 1-dimensional images\" (num_images, height * width)."
      ],
      "metadata": {
        "id": "Vl-0UxIJxTkn",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_train.dtype)\n",
        "print(X_train.min(), X_train.max())"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "B-e6X4v6wlJM",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to convert that to `torch` tensors and reshape the input. Also, it's a good idea to normalize your image data to lie within a $[0, 1]$ interval. Let's write a function that does all these things:"
      ],
      "metadata": {
        "id": "XF2nptPHx3-g",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a function to convert X and y to torch tensors while\n",
        "# rescaling X to fit into [0, 1] interval and reshaping it properly\n",
        "\n",
        "# Hint: make sure your input tensor dtype is same as the\n",
        "# parameters of the model (should be torch.float)\n",
        "\n",
        "def preprocess_data(X, y):\n",
        "  X_preprocessed = <YOUR CODE HERE>\n",
        "  y_preprocessed = <YOUR CODE HERE>\n",
        "  return X_preprocessed, y_preprocessed"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "TMiuBEH42Hso",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest way to check this function is to use it to calculate loss:"
      ],
      "metadata": {
        "id": "ZIUhQLYN_4mL",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_X, processed_y = preprocess_data(X_train[:100], y_train[:100])\n",
        "\n",
        "assert processed_X.min().item() >= 0 and \\\n",
        "       processed_X.max().item() <= 1, \"Make sure your input is >= 0 and <= 1\"\n",
        "\n",
        "class_scores = model(processed_X)\n",
        "print(\"Loss is:\", loss_function(class_scores, processed_y).item())"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "VgR5FZus3CRW",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer"
      ],
      "metadata": {
        "id": "u2nybq16Ak7a",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, to train our network we need an optimization algorithm. \n",
        "\n",
        "Actually, we already implemented one ourselves: stochastic gradient descent (from the *Automatic gradients* section of this notebook). Back then we manually updated our model's parameters by adding a `-learning_rate * param.grad` at each step.\n",
        "\n",
        "There are however other extensions of stochastic gradient descent that compute the per-step updates using more complicated functions of gradients (some take into account the gradients' values at earlier steps).\n",
        "\nAs always, many of those are already implemented in torch. In this example we are going to use `torch.optim.Adam`."
      ],
      "metadata": {
        "id": "x4HrUU_rA_M1",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.005\n",
        "# When creating an istance of the optimizer you have to tell\n",
        "# it which parameters you want to optimize. We are doing so by\n",
        "# passing model.parameters() to it:\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "RtlkvLG6F3XK",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once an optimizer is created we can use it by first calculating the loss on some batch of data, computing the gradients and then simply calling to `optimizer.step()`. Then repeat with the next batch and so on.\n",
        "\nHere's an example of what should happen at a single step:"
      ],
      "metadata": {
        "id": "jrBk5DCOGul-",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of data (we'll get first 100 images at first)\n",
        "batch_X, batch_y = preprocess_data(X_train[:100], y_train[:100])\n",
        "\n",
        "# Compute the loss:\n",
        "loss = loss_function(model(batch_X), batch_y)\n",
        "\n",
        "print(\"Loss value before the optimization step:\", loss.item())\n",
        "\n",
        "# Zero the gradients\n",
        "model.zero_grad()\n",
        "\n",
        "# Compute new gradients\n",
        "loss.backward()\n",
        "\n",
        "# Do the optimization step:\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Loss value after the optimization step:\",\n",
        "      loss_function(model(batch_X), batch_y).item())"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "X224ZlG-GAq1",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some utilities"
      ],
      "metadata": {
        "id": "NL1rGhZUxshQ",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ideally we would like to loop through all of our train data. One such loop is called an 'epoch'. We'd also like the data to be shuffled at each epoch.\n",
        "\nLet's write a function that gives us batches of data and which we'll call at the start of each epoch:"
      ],
      "metadata": {
        "id": "0J4oPwJYkSYG",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch generator\n",
        "# (here's a very brief description of what python generators are:\n",
        "# https://stackoverflow.com/a/231855/3801744)\n",
        "def get_batches(X, y, batch_size, shuffle=False):\n",
        "  if shuffle:\n",
        "    shuffle_ids = np.random.permutation(len(X))\n",
        "    X = X[shuffle_ids].copy()\n",
        "    y = y[shuffle_ids].copy()\n",
        "  for i_picture in range(0, len(X), batch_size):\n",
        "    # Get batch and preprocess it:\n",
        "    batch_X = X[i_picture:i_picture + batch_size]\n",
        "    batch_y = y[i_picture:i_picture + batch_size]\n",
        "    \n",
        "    # 'return' the batch (see the link above to\n",
        "    # better understand what 'yield' does)\n",
        "    yield preprocess_data(batch_X, batch_y)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "FKlNWpZ5kOQX",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll add one more utility to keep track of loss values and plot them at each epoch:"
      ],
      "metadata": {
        "id": "zDLRDuFVrzX9",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "class Logger:\n",
        "  def __init__(self):\n",
        "    self.train_loss_batch = []\n",
        "    self.train_loss_epoch = []\n",
        "    self.test_loss_batch = []\n",
        "    self.test_loss_epoch = []\n",
        "    self.train_batches_per_epoch = 0\n",
        "    self.test_batches_per_epoch = 0\n",
        "    self.epoch_counter = 0\n",
        "\n",
        "  def fill_train(self, loss):\n",
        "    self.train_loss_batch.append(loss)\n",
        "    self.train_batches_per_epoch += 1\n",
        "\n",
        "  def fill_test(self, loss):\n",
        "    self.test_loss_batch.append(loss)\n",
        "    self.test_batches_per_epoch += 1\n",
        "\n",
        "  def finish_epoch(self):\n",
        "    self.train_loss_epoch.append(np.mean(\n",
        "        self.train_loss_batch[-self.train_batches_per_epoch:]\n",
        "    ))\n",
        "    self.test_loss_epoch.append(np.mean(\n",
        "        self.test_loss_batch[-self.test_batches_per_epoch:]\n",
        "    ))\n",
        "    self.train_batches_per_epoch = 0\n",
        "    self.test_batches_per_epoch = 0\n",
        "    \n",
        "    clear_output()\n",
        "  \n",
        "    print(\"epoch #{} \\t train_loss: {:.8} \\t test_loss: {:.8}\".format(\n",
        "              self.epoch_counter,\n",
        "              self.train_loss_epoch[-1],\n",
        "              self.test_loss_epoch [-1]\n",
        "          ))\n",
        "    \n",
        "    self.epoch_counter += 1\n",
        "\n",
        "    plt.figure(figsize=(11, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(self.train_loss_batch, label='train loss')\n",
        "    plt.xlabel('# batch iteration')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(self.train_loss_epoch, label='average train loss')\n",
        "    plt.plot(self.test_loss_epoch , label='average test loss' )\n",
        "    plt.legend()\n",
        "    plt.xlabel('# epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.show();"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "P6XVWw6on2A_",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting it all together"
      ],
      "metadata": {
        "id": "x_2R-RM6Am3C",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the setting up part, copy-pasted from previous cells:"
      ],
      "metadata": {
        "id": "SCNSaIcwmEKU",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the loss function:\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Defining the model\n",
        "input_size = 28 * 28 # number of pixels per image\n",
        "output_size = 10 # number of classes\n",
        "model = torch.nn.Linear(in_features=input_size,\n",
        "                        out_features=output_size)\n",
        "\n",
        "# Setting up the optimizer\n",
        "learning_rate = 0.005\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "Qv6R9kqbl5Of",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's your turn to code the learning process:"
      ],
      "metadata": {
        "id": "2XvdiR4-m8DH",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 30\n",
        "batch_size=100\n",
        "\n",
        "logger = Logger()\n",
        "\n",
        "for i_epoch in range(n_epochs):\n",
        "  for batch_X, batch_y in get_batches(X_train, y_train,\n",
        "                                      batch_size=batch_size,\n",
        "                                      shuffle=True):\n",
        "\n",
        "    # Compute the loss, zero the gradients, and make an optimization step\n",
        "    <YOUR CODE HERE>\n",
        "\n",
        "    logger.fill_train(loss.item())\n",
        "\n",
        "  for batch_X, batch_y in get_batches(X_test, y_test,\n",
        "                                      batch_size=batch_size):\n",
        "\n",
        "    # Compute the loss\n",
        "    <YOUR CODE>\n",
        "\n",
        "    logger.fill_test(loss.item())\n",
        "\n  logger.finish_epoch()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "4U3YMWEUvL6T",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've trained our model, let's evaluate the accuracy of the predictions. Here's a function we can use to compute the predictions on the test set:"
      ],
      "metadata": {
        "id": "QmRh1KUNv6LC",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_predictions(model, batch_size=100):\n",
        "  predictions_test = np.concatenate([\n",
        "    model(batch_X).to('cpu').detach().numpy()\n",
        "    for batch_X, batch_y in get_batches(X_test, y_test, batch_size)\n",
        "  ], axis=0)\n",
        "  return np.argmax(predictions_test, axis=1)\n",
        "\nfrom sklearn.metrics import accuracy_score"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "qSEGq892e9qZ",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use it to calculate the accuracy score here. How much did you get?"
      ],
      "metadata": {
        "id": "KOVlfwsQyqw5",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and print out the test accuracy of your model\n",
        "<YOUR CODE HERE>"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "iZDCk6tGy_Ae",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, so what we did so far is just a simple logistic regression model. Let's add a hidden layer to turn it into a 'real' neural net. Modify the code above.\n",
        "\n",
        "You can stack layers by using `torch.nn.Sequential` class:\n",
        "\n",
        "```\n",
        "model = torch.nn.Sequential(\n",
        "  torch.nn.Linear(...),\n",
        "  torch.nn.ELU(),\n",
        "  torch.nn.Linear(...),\n",
        ")\n",
        "```\n",
        "\n",
        "In this example we've put an activation layer `torch.nn.ELU` between the two linear ones. What will happen if we don't do that?\n",
        "\n",
        "If your model starts to overfit, try adding a `torch.nn.Dropout` layer after the hidden layer. Dropout layers behave differently in training and evaluating modes, so you have specify it like this:\n",
        "\n",
        "```\n",
        "model.train() # set to training mode\n",
        "# ...\n",
        "# (do the training)\n",
        "model.eval() # set to evaluating mode\n",
        "# ...\n",
        "# (perform evaluation)\n",
        "```"
      ],
      "metadata": {
        "id": "SZoCgsSoPWVZ",
        "colab_type": "text"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "file_extension": ".py",
      "version": "3.5.2",
      "nbconvert_exporter": "python"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}