{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearModels_ToyData.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatMISiS2018/blob/master/02_lab/01-LinearModels_ToyData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "eLCHEg3scvrR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear models"
      ]
    },
    {
      "metadata": {
        "id": "3fvCUTeNdSMX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Linear regression"
      ]
    },
    {
      "metadata": {
        "id": "52bVl12VwKi9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For a set of inputs\n",
        "$$({\\bf x}_i, y_i),$$\n",
        "$${\\bf x}_i\\in \\mathbb{R}^M,$$\n",
        "$$y_i\\in \\mathbb{R}^1,$$\n",
        "$$i=1,\\ldots,N$$\n",
        "we are looking for a function of the form\n",
        "$$f({\\bf x}) = {\\bf W}\\cdot{\\bf x} + b,$$\n",
        "$${\\bf W}\\in \\mathbb{R}^M,$$\n",
        "$$b\\in \\mathbb{R}^1$$\n",
        "such that\n",
        "$$\\sum_{i=1}^N(y_i - f({\\bf x}_i))^2$$\n",
        "is minimal."
      ]
    },
    {
      "metadata": {
        "id": "-aC9KrjJvb26",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's do our regular imports:"
      ]
    },
    {
      "metadata": {
        "id": "hqN_z3MoAmcd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NxueISKKvfYk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are going to use a linear regression model to fit a 3rd order polynomial."
      ]
    },
    {
      "metadata": {
        "id": "olbCcZAINQJH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nx = 15 # number of points\n",
        "np.random.seed(42) # to reproduce the same random state\n",
        "x = np.sort(np.random.uniform(-2., 2., size=nx))\n",
        "y = x**3 - 1.5 * x**2 - 2. * x - 1\n",
        "y += np.random.normal(scale=0.7, size=nx) # adding some noize to the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "75NB8gDdNQHf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(x, y, 'o', markersize=6);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tVXioLj7z7Ic",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will use the scikit-learn library (https://scikit-learn.org/). Let's import its `linear_model` module:"
      ]
    },
    {
      "metadata": {
        "id": "tVdImjw_NQD6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AWkizxBN0W5G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`sklearn` models expect that our input to be of shape `(n_samples, n_features)`. So far we have only one single feature, so we have to reshape our input to `(n_samples, 1)`. Let's write a function for that:"
      ]
    },
    {
      "metadata": {
        "id": "wkV_wjX-RIkT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def features(x):\n",
        "  \"\"\"\n",
        "  Function that takes 1-dimensional array x and returns\n",
        "  2-dimensional array of the shape (n_samples, n_features)\n",
        "  \"\"\"\n",
        "  <YOUR CODE HERE>\n",
        "\n",
        "assert(features(x).shape == (nx, 1))\n",
        "print(features(x).shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "91O-i5v8060I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ok, now it's time to build and train the model:"
      ]
    },
    {
      "metadata": {
        "id": "Kgp55zOzNQAc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = linear_model.LinearRegression()\n",
        "model.fit(features(x), y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ky28C-Qo0-eh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's write a function to plot the results:"
      ]
    },
    {
      "metadata": {
        "id": "EzFRuUmMiZLh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_model(model, features):\n",
        "  \"\"\"\n",
        "  Function that plots the data points and\n",
        "  model prediction\n",
        "  \"\"\"\n",
        "  plt.plot(x, y, 'o', markersize=6);\n",
        "  \n",
        "  # Hint: use model.predict(features(<ARRAY OF X>))\n",
        "  # to get an array of predictions\n",
        "  <YOUR CODE HERE>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yliIY9ski7j1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_model(model, features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GtxZNIgJ2n0o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Obviously, linear model can't fit a non-linear dependency. Or can it?\n",
        "\n",
        "Can you think of a modification to the inputs such that our model turns to a higher degree polynomial fit?"
      ]
    },
    {
      "metadata": {
        "id": "V2nakssXSCA5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def features(x):\n",
        "  \"\"\"\n",
        "  Function that takes 1-dimensional vector x and returns\n",
        "  2-dimensional array of the shape (n_samples, n_features)\n",
        "  with non-linear features\n",
        "  \"\"\"\n",
        "  <YOUR CODE HERE>\n",
        "\n",
        "model = linear_model.LinearRegression()\n",
        "model.fit(features(x), y)\n",
        "\n",
        "plot_model(model, features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zcmOlVq8Tn09",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def features(x):\n",
        "  \"\"\"\n",
        "  Function that takes 1-dimensional vector x and returns\n",
        "  2-dimensional array of the shape (n_samples, n_features)\n",
        "  with non-linear features (power 1, 2 and 3)\n",
        "  \"\"\"\n",
        "  <YOUR CODE HERE>\n",
        "\n",
        "model = linear_model.LinearRegression()\n",
        "model.fit(features(x), y)\n",
        "\n",
        "plot_model(model, features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H1-H49rB3dLh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3rd order polynomial fit looks good! Can we go higher?"
      ]
    },
    {
      "metadata": {
        "id": "K_dTuOq4TnxU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def features(x):\n",
        "  \"\"\"\n",
        "  Function that takes 1-dimensional vector x and returns\n",
        "  2-dimensional array of the shape (n_samples, n_features)\n",
        "  with non-linear features of high powers (12?)\n",
        "  \"\"\"\n",
        "  <YOUR CODE HERE>\n",
        "\n",
        "model = linear_model.LinearRegression()\n",
        "model.fit(features(x), y)\n",
        "\n",
        "plot_model(model, features)\n",
        "plt.ylim(-20, 20);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i7r8LhUW3xro",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Hmm, is this fit better? It certainly goes closer to the input points...  Let's test its predictive power:"
      ]
    },
    {
      "metadata": {
        "id": "36cnghYFTnqM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "selection = np.random.binomial(1, 0.7, len(x)).astype(bool)\n",
        "\n",
        "model = linear_model.LinearRegression()\n",
        "model.fit(features(x[selection]), y[selection])\n",
        "\n",
        "plot_model(model, features)\n",
        "plt.ylim(-20, 20);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vrAN1wpK4Mhu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So it doesn't describe the points it has never seen. This is called **overfitting**."
      ]
    },
    {
      "metadata": {
        "id": "3uB5eJrd4rmq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## k-fold cross-validation"
      ]
    },
    {
      "metadata": {
        "id": "sHC7vvEV4vSX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is a common validation method:\n",
        "  - split the dataset $D$ to $k$ parts: $D_i$, $i=1,\\ldots,k$\n",
        "  - for each $i=1,\\ldots,k$ do the following:\n",
        "    - fit the model on $D \\setminus D_i$ (on all elements from $D$ that are not in $D_i$)\n",
        "    - evaluate model performance on $D_i$: obtain score $l_i$\n",
        "  - calculate the average score $l=\\frac{1}{k}\\sum_{i=1}^kl_i$ of the model's performance."
      ]
    },
    {
      "metadata": {
        "id": "a1r5nvFV9M7f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's assume we don't know the nature of our input data and we want to select the best model. We can use k-fold cross-validation method to evaluate our model for each polynomial order.\n",
        "\n",
        "Scikit-learn has the `model_selection` module with k-fold x-validation functionality implemented."
      ]
    },
    {
      "metadata": {
        "id": "nctH99xdTnmj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection\n",
        "\n",
        "\n",
        "# defining our score function (root mean square loss - the smaller the better)\n",
        "def rms_loss(y_true, y_pred):\n",
        "  return <YOUR CODE HERE>\n",
        "\n",
        "assert np.isclose(rms_loss(np.arange(10), np.linspace(-3, 80, 10)), 41.39735185661929), \"Check your rms_loss function\"\n",
        "\n",
        "\n",
        "# train and evaluate the model on a single split\n",
        "def single_eval(x_train, y_train, x_test, y_test, features):\n",
        "  model = linear_model.LinearRegression()\n",
        "  model.fit(features(x_train), y_train)\n",
        "  return rms_loss(y_test, model.predict(features(x_test)))\n",
        "\n",
        "\n",
        "# run the k-fold x-validation loop and return the array of losses\n",
        "def test_model(features, n_splits=5):\n",
        "  kf = model_selection.KFold(n_splits=n_splits, shuffle=True, random_state=1234)\n",
        "  rms = []\n",
        "  for i_train, i_test in kf.split(x):\n",
        "    rms.append(single_eval(x[i_train], y[i_train], x[i_test], y[i_test], features))\n",
        "  return np.array(rms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4KPRN70cTni5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rms_vs_power = [\n",
        "    (i, test_model(\n",
        "          lambda x: np.stack([x**n for n in range(1, i + 1)]).T\n",
        "    )) for i in range(1, 14)\n",
        "]\n",
        "\n",
        "powers, rmss = zip(*rms_vs_power)\n",
        "rmss = np.array(rmss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a9Ni35OqTnfn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.fill_between(x=powers,\n",
        "                 y1=rmss.min(axis=1),\n",
        "                 y2=rmss.max(axis=1),\n",
        "                 alpha=0.3\n",
        "                );\n",
        "\n",
        "plt.errorbar(x=powers,\n",
        "             y   =(rmss.max (axis=1) + rmss.min (axis=1)) * 0.5,\n",
        "             yerr=(rmss.max (axis=1) - rmss.min (axis=1)) * 0.5,\n",
        "             fmt=' ');\n",
        "\n",
        "plt.yscale('log')\n",
        "plt.xlabel('order of the polynomial')\n",
        "plt.ylabel('rms range');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S_Iv3iCw_ZF5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Logistic regression"
      ]
    },
    {
      "metadata": {
        "id": "CQGS0dRmmuym",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Binary classification:\n",
        "$$y_i\\in\\left\\{0; 1\\right\\}$$\n",
        "\n",
        "Model:\n",
        "$$P(y=1|{\\bf x}, {\\bf W}) = \\sigma({\\bf W}\\cdot{\\bf x}),$$\n",
        "where $\\sigma$ is the *sigmoid* function:\n",
        "$$\\sigma(a) = \\frac{1}{1 + e^{-a}}$$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "c5IWM7pxnulk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  \"\"\"\n",
        "  Sigma function (from the definition above)\n",
        "  \"\"\"\n",
        "  <YOUR CODE HERE>\n",
        "\n",
        "x = np.linspace(-10, 10, 100)\n",
        "plt.plot(x, sigmoid(x));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R3XV72qbr5S2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This model is implemented in scikit-learn with the following default loss (cross-entropy):\n",
        "$$L({\\bf W})=-\\sum_i\\left(\n",
        "y_i\\, log P(y=1|{\\bf x}_i, {\\bf W})\n",
        "+\n",
        "(1-y_i)logP(y=0|{\\bf x}_i, {\\bf W})\n",
        "\\right)$$\n",
        "plus a regularization term $\\frac{1}{2}{\\bf W}\\cdot{\\bf W}$.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
      ]
    },
    {
      "metadata": {
        "id": "8bvhqE_53r4e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's try to separate two rectangular clusters with it."
      ]
    },
    {
      "metadata": {
        "id": "Bd0l1geF3ls9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N0 = 512\n",
        "N1 = 512\n",
        "\n",
        "X0_ = np.random.uniform(size=(N0, 2)) * [[2, 1]] + np.random.normal(scale=0.05, size=(N0, 2)) + [[0, 1]]\n",
        "X1_ = np.random.uniform(size=(N1, 2)) * [[2, 1]] + np.random.normal(scale=0.05, size=(N0, 2))\n",
        "\n",
        "\n",
        "alpha = np.pi / 6.\n",
        "rot = [[ np.cos(alpha), np.sin(alpha)],\n",
        "       [-np.sin(alpha), np.cos(alpha)]]\n",
        "X0 = np.matmul(rot, X0_.T).T\n",
        "X1 = np.matmul(rot, X1_.T).T\n",
        "\n",
        "y0 = np.zeros(shape=N0)\n",
        "y1 = np.ones (shape=N1)\n",
        "\n",
        "X = np.concatenate([X0, X1], axis=0)\n",
        "y = np.concatenate([y0, y1], axis=0)\n",
        "perm = np.random.permutation(len(y))\n",
        "X = X[perm]\n",
        "y = y[perm]\n",
        "\n",
        "plt.scatter(*X[y==0].T);\n",
        "plt.scatter(*X[y==1].T);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PRWPq5Iw7_tb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lwo3lH_T7All",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25)\n",
        "model = linear_model.LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "preds_train = model.predict_proba(X_train)[:,1]\n",
        "preds_test  = model.predict_proba(X_test )[:,1]\n",
        "\n",
        "print(\"train score: {}\".format(metrics.roc_auc_score(y_train, preds_train)))\n",
        "print(\"test score:  {}\".format(metrics.roc_auc_score(y_test , preds_test )))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y-S13kWQv4Q0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ROC-curve and AUC"
      ]
    },
    {
      "metadata": {
        "id": "6HLFlXETw_Ak",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We've just used `roc_auc_score` function to evaluate our model. What is that?\n",
        "\n",
        "Consider the output discriminating function $p^\\hat{y}$ our model produces (class 1 probability):"
      ]
    },
    {
      "metadata": {
        "id": "0i7QBOgYv7ff",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bins = np.linspace(-0., 1., 31)\n",
        "plt.hist(preds_test[y_test == 0], bins=bins, alpha=0.8, label='class 0')\n",
        "plt.hist(preds_test[y_test == 1], bins=bins, alpha=0.8, label='class 1')\n",
        "plt.xlabel('$p^\\hat{y}$')\n",
        "plt.legend();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kitEoKWlxjft",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If we want to select only class 1 events, we can introduce a threshold $T$ and only select events with class 1 probability greater than $T$:\n",
        "$$p^\\hat{y} > T.$$\n",
        "For some value of $T$ false positive rate is defined as:\n",
        "$$\\text{FPR}(T) = P(p^\\hat{y} > T | y=0),$$\n",
        "i.e. probability for the class 0 component to exceed the threshold. Similarly, true positive rate is defined as:\n",
        "$$\\text{TPR}(T) = P(p^\\hat{y} > T | y=1).$$\n",
        "\n",
        "Now, if we plot TPR versus FPR for all possible values of $T$ we'll get the **receiver operating characteristic**, or simply **ROC-curve**:"
      ]
    },
    {
      "metadata": {
        "id": "JxSmsQCkwndp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fpr, tpr, _ = metrics.roc_curve(y_test, preds_test)\n",
        "\n",
        "plt.plot(fpr, tpr);\n",
        "plt.xlabel('false positive rate')\n",
        "plt.ylabel('true positive rate');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dh2APNZ60KnQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This curve shows how well our model can separate the classes. To convert it to a single number we calculated the **area ander the ROC-curve** (AUC). The grater this area is, the better separation we get.\n",
        "\n",
        "By the way, AUC also equals to the probability that the discriminating variable value for a random class 1 object is greater than such value for a random class 0 object:\n",
        "$$\\text{AUC} = P(p^{\\hat{y}}_a > p^{\\hat{y}}_b|y_a=1,y_b=0)$$"
      ]
    },
    {
      "metadata": {
        "id": "aVnTBxa6199g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let's write a function that plots a 2d map of our model's predictions:"
      ]
    },
    {
      "metadata": {
        "id": "SMFn8xBik6wj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import colors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m7DXXM3e8c8d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "k = 0.85\n",
        "thr1 = 0.25\n",
        "thr2 = 1. - thr1\n",
        "\n",
        "thrs = np.array([0., thr1, 0.5, thr2, 1.0])\n",
        "blue  = np.array([0.5, 0.75, 1.])\n",
        "white = np.array([1., 1., 1.])\n",
        "green = np.array([0.5, 1., 0.75])\n",
        "cm_colors = np.stack([green * k,\n",
        "                      green,\n",
        "                      white,\n",
        "                      blue,\n",
        "                      blue * k][::-1], axis=1)\n",
        "\n",
        "cmap = colors.LinearSegmentedColormap('custom_cmap_blue2green', {    \n",
        "         'red'   :  np.stack([thrs, cm_colors[0], cm_colors[0]], axis=1),\n",
        "         'green' :  np.stack([thrs, cm_colors[1], cm_colors[1]], axis=1),\n",
        "         'blue'  :  np.stack([thrs, cm_colors[2], cm_colors[2]], axis=1)})\n",
        "\n",
        "def visualize_model(\n",
        "        model,\n",
        "        grid0=np.linspace(-0.5, 3.0, 100),\n",
        "        grid1=np.linspace(-1.5, 2.0, 100),\n",
        "        feature_transform=lambda x: x\n",
        "    ):\n",
        "  x0, x1 = np.meshgrid(grid0, grid1)\n",
        "  xx = np.stack([x0.flatten(), x1.flatten()], axis=1)\n",
        "  y = model.predict_proba(feature_transform(xx))[:,1].reshape(x0.shape)\n",
        "  cnt = plt.contourf(x0, x1, y, 100, cmap=cmap)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wIRyxT8o9jrV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7, 7))\n",
        "visualize_model(model)\n",
        "plt.scatter(*X[y==0].T);\n",
        "plt.scatter(*X[y==1].T);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1hXqNcoEuwli",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Good. Now, we'll try a more complicated topology: concentric rings."
      ]
    },
    {
      "metadata": {
        "id": "OfGAoWopg_FO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aoi9oe5Eg-0z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X, y = datasets.make_circles(n_samples=10000, shuffle=True, noise=0.2, factor=0.4)\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.scatter(*X[y==0].T, s=10);\n",
        "plt.scatter(*X[y==1].T, s=10);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RmVPQ7x2na4k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = linear_model.LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "visualize_model(model,\n",
        "                np.linspace(-2., 2., 100),\n",
        "                np.linspace(-2., 2., 100));\n",
        "plt.scatter(*X[y==0].T, s=10);\n",
        "plt.scatter(*X[y==1].T, s=10);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wIaSc9A0vBqa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Again, linear model doesn't seem to be good enough to describe the data. Or does it?"
      ]
    },
    {
      "metadata": {
        "id": "3ppxP9C4na0w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def features(xx):\n",
        "  \"\"\"\n",
        "  Function that transforms features array of shape (n_sample, n_features)\n",
        "  adding non-linear features\n",
        "  \"\"\"\n",
        "  <YOUR CODE HERE>\n",
        "\n",
        "model = linear_model.LogisticRegression()\n",
        "model.fit(features(X), y)\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "visualize_model(model,\n",
        "                np.linspace(-2., 2., 100),\n",
        "                np.linspace(-2., 2., 100),\n",
        "                features)\n",
        "plt.scatter(*X[y==0].T, s=10);\n",
        "plt.scatter(*X[y==1].T, s=10);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YCT1S_HF5g1w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def single_eval(x_train, y_train, x_test, y_test, features, plot=False):\n",
        "  model = linear_model.LogisticRegression()\n",
        "  model.fit(features(x_train), y_train)\n",
        "  preds_test = model.predict_proba(features(x_test))[:,1]\n",
        "  if plot:\n",
        "    fpr, tpr, _ = metrics.roc_curve(y_test, preds_test)\n",
        "    plt.plot(fpr, tpr)\n",
        "  return metrics.roc_auc_score(y_test, preds_test)\n",
        "\n",
        "def test_model(features, n_splits=30, plot=False):\n",
        "  kf = model_selection.KFold(n_splits=n_splits, shuffle=True, random_state=1234)\n",
        "  aucs = []\n",
        "  for i_train, i_test in kf.split(X):\n",
        "    aucs.append(single_eval(X[i_train], y[i_train], X[i_test], y[i_test], features, plot=plot))\n",
        "  return np.array(aucs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VitHJRgw7mvH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N = 20\n",
        "aucs = test_model(features, n_splits=N, plot=True)\n",
        "print(\"AUC = {:.4f} +\\- {:.4f}\".format(aucs.mean(), aucs.std() / N**0.5))\n",
        "plt.xlim(0, 0.4)\n",
        "plt.ylim(0.6, 1)\n",
        "plt.xlabel('fpr')\n",
        "plt.ylabel('tpr')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qCVw8qYz8dXd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7, 0.7))\n",
        "plt.scatter(aucs, [0] * len(aucs), s=100, alpha=0.5, c='r');\n",
        "plt.yticks([]);\n",
        "plt.xlabel(\"AUC\");"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}